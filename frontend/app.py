import streamlit as st
import requests
import base64
import io
import json
import time
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import os
import tempfile
import base64
import google.generativeai as genai
from PIL import Image
from ultralytics import YOLO
from dotenv import load_dotenv
from streamlit_option_menu import option_menu

load_dotenv()
GEMINI_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_KEY:
    try:
        genai.configure(api_key=GEMINI_KEY)
    except Exception:
        # proceed: genai may still error at call time
        pass

st.set_page_config(page_title="Nh·∫≠n d·∫°ng v√† ph√¢n lo·∫°i ƒë·ªô ch√≠n tr√°i m√≠t", layout="wide")
st.markdown(
    """
    <style>
    .main-title {
        font-size: 20px;
        font-weight: 800;
        text-align: center;
        color: #2E7D32;
        margin-bottom: 0.3em;
        letter-spacing: 0.5px;
    }
    .sub-title {
        text-align: center;
        font-size: 18px;
        color: #555;
        font-style: italic;
        margin-bottom: 1.5em;
    }
    hr {
        border: none;
        height: 2px;
        background: linear-gradient(to right, #8BC34A, #558B2F);
        margin-bottom: 1.5em;
    }
    </style>
    
    <div class="main-title">·ª®NG D·ª§NG YOLOv8 TRONG NH·∫¨N D·∫†NG V√Ä PH√ÇN LO·∫†I ƒê·ªò CH√çN TR√ÅI M√çT</div>
    <p class="sub-title">Ph·ª•c v·ª• N√¥ng nghi·ªáp th√¥ng minh üåæ</p>
    <hr>
    """,
    unsafe_allow_html=True
)

ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
API_URL = "http://127.0.0.1:8000/predict"
LATEST_RESULTS = os.path.join(os.path.dirname(__file__), "latest_results.json")

# --- Chuy·ªÉn ·∫£nh logo sang base64 ƒë·ªÉ hi·ªÉn th·ªã ---
def get_base64_of_bin_file(bin_file):
    with open(bin_file, "rb") as f:
        data = f.read()
    return base64.b64encode(data).decode()

logo_path = os.path.join(os.path.dirname(__file__), "logo.png")
if os.path.exists(logo_path):
    logo_base64 = get_base64_of_bin_file(logo_path)
    logo_html = f'<img src="data:image/png;base64,{logo_base64}" width="100" style="border-radius:10px; margin-bottom:10px"/>'
else:
    logo_html = "<div style='font-size:40px'>üçà</div>"


theme = st.get_option("theme.base")  # tr·∫£ v·ªÅ 'dark' ho·∫∑c 'light'

if theme == "dark":
    menu_styles = {
        "container": {
            "background-color": "#1C1E24",
            "padding": "1rem",
            "border-radius": "12px",
        },
        "icon": {"color": "#FFFFFF", "font-size": "20px"},
        "nav-link": {
            "font-size": "16px",
            "color": "#FFFFFFCC",
            "text-align": "left",
            "margin": "6px 0",
            "--hover-color": "#292B33",
            "border-radius": "8px",
        },
        "nav-link-selected": {
            "background-color": "#6DBE45",
            "color": "#FFFFFF",
            "font-weight": "600",
        },
    }
else:
    menu_styles = {
        "container": {
            "background-color": "#FFFFFF",
            "padding": "1rem",
            "border-radius": "12px",
            "box-shadow": "0 2px 8px rgba(0,0,0,0.05)",
        },
        "icon": {"color": "#8EEB60", "font-size": "20px"},
        "nav-link": {
            "font-size": "16px",
            "color": "#000000CC",
            "text-align": "left",
            "margin": "6px 0",
            "--hover-color": "#E8F5E9",
            "border-radius": "8px",
        },
        "nav-link-selected": {
            "background-color": "#6DBE45",
            "color": "#FFFFFF",
            "font-weight": "600",
        },
    }
# Sidebar logo + menu
with st.sidebar:
    st.markdown(
        f"""
        <div style="text-align:center; padding-bottom:10px">
             {logo_html}
            <h3 style="margin:0; color:#6DBE45;">üåæ N√¥ng nghi·ªáp th√¥ng minh</h3>
            <p style="font-size:13px; color:gray;">·ª®ng d·ª•ng AI nh·∫≠n d·∫°ng ƒë·ªô ch√≠n tr√°i m√≠t</p>
        </div>
        """,
        unsafe_allow_html=True,
    )

    choice = option_menu(
        None,
        ["Trang ch·ªß", "Nh·∫≠n d·∫°ng ·∫£nh", "Video/Webcam",
         "Th·ªëng k√™", "So s√°nh YOLOv8", "AI Insight", "Chat Gemini"],
        icons=["house", "camera", "camera-video", "bar-chart", "activity", "cpu", "chat-dots"],
        default_index=1,
        styles=menu_styles,
    )

if choice == "Trang ch·ªß":
    st.markdown("""
    ### üéØ M·ª•c ti√™u d·ª± √°n
    ·ª®ng d·ª•ng AI gi√∫p n√¥ng d√¢n nh·∫≠n bi·∫øt **ƒë·ªô ch√≠n c·ªßa tr√°i m√≠t** qua h√¨nh ·∫£nh, 
    h·ªó tr·ª£ **ra quy·∫øt ƒë·ªãnh thu ho·∫°ch ch√≠nh x√°c**, gi·∫£m th·∫•t tho√°t, 
    h∆∞·ªõng ƒë·∫øn **n√¥ng nghi·ªáp th√¥ng minh**.
    """)
    st.info("Ch·ªçn m·ª•c trong menu b√™n tr√°i ƒë·ªÉ b·∫Øt ƒë·∫ßu üëâ")

# ---------------- TAB 1: ·∫¢NH ----------------
elif choice == "Nh·∫≠n d·∫°ng ·∫£nh":
    st.header("Nh·∫≠n d·∫°ng ·∫£nh tƒ©nh")
    col1, col2 = st.columns([1,1])
    with col1:
        uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh tr√°i m√≠t...", type=["jpg","jpeg","png"])
        confidence = st.slider("Ng∆∞·ª°ng confidence", 0.1, 1.0, 0.5, 0.05)
        enable_alert = st.checkbox("üîî B·∫≠t c·∫£nh b√°o thu ho·∫°ch qua email (gi·∫£ l·∫≠p)", value=False)
        analyze_btn = st.button("üîç Ph√¢n t√≠ch ·∫£nh")

    with col2:
        st.markdown("**·∫¢nh g·ªëc**")
        preview = st.empty()
        st.markdown("**·∫¢nh k·∫øt qu·∫£**")
        out_image = st.empty()
        st.markdown("**Th·ªëng k√™ nhanh**")
        stats_box = st.empty()

    if uploaded_file:
        img = Image.open(uploaded_file).convert("RGB")
        preview.image(img, use_container_width=True)

    if analyze_btn and uploaded_file:
        with st.spinner("G·ª≠i ·∫£nh t·ªõi API, ch·ªù k·∫øt qu·∫£..."):
            files = {"file": uploaded_file.getvalue()}
            try:
                resp = requests.post(API_URL, files=files, params={"conf": confidence}, timeout=30)
                resp.raise_for_status()
                data = resp.json()
            except Exception as e:
                st.error(f"L·ªói g·ªçi API: {e}")
                data = None

        if data:
            # show annotated image
            img_data = base64.b64decode(data["image"])
            annotated = Image.open(io.BytesIO(img_data)).convert("RGB")
            out_image.image(annotated, use_container_width=True)

            detections = data.get("detections", [])
            if not detections:
                stats_box.warning("Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c tr√°i m√≠t n√†o.")
            else:
                # b·∫£ng detections
                df = pd.DataFrame(detections)
                stats_box.dataframe(df)

                # counts + pie chart
                labels = df["label"].tolist()
                count_df = pd.Series(labels).value_counts().rename_axis('Lo·∫°i').reset_index(name='S·ªë l∆∞·ª£ng')
                st.subheader("Th·ªëng k√™")
                st.dataframe(count_df)

                fig, ax = plt.subplots()
                ax.pie(count_df["S·ªë l∆∞·ª£ng"], labels=count_df["Lo·∫°i"], autopct="%1.1f%%", startangle=90)
                ax.axis("equal")
                st.pyplot(fig)

                # g·ª£i √Ω thu ho·∫°ch
                total = count_df["S·ªë l∆∞·ª£ng"].sum()
                chin = int(count_df[count_df["Lo·∫°i"]=="mit_chin"]["S·ªë l∆∞·ª£ng"].sum()) if "mit_chin" in count_df["Lo·∫°i"].values else 0
                ratio = chin / total if total>0 else 0
                if ratio > 0.7:
                    st.success("üåæ G·ª£i √Ω: H∆°n 70% tr√°i ch√≠n ‚Äî n√™n thu ho·∫°ch s·ªõm.")
                    if enable_alert:
                        st.info("üì§ [Gi·∫£ l·∫≠p] ƒê√£ g·ª≠i email c·∫£nh b√°o t·ªõi user@example.com")
                elif ratio > 0.4:
                    st.info("üçà Nhi·ªÅu tr√°i ƒëang ch√≠n ‚Äî theo d√µi th√™m.")
                else:
                    st.warning("üü¢ Ch·ªß y·∫øu l√† m√≠t non, ch∆∞a n√™n thu ho·∫°ch.")

            # write latest_results.json was created by backend; just show
            if os.path.exists(LATEST_RESULTS):
                st.info("‚úÖ K·∫øt qu·∫£ l∆∞u s·∫µn cho AI Insight.")
            else:
                st.info("‚ö†Ô∏è latest_results.json ch∆∞a ƒë∆∞·ª£c l∆∞u (ki·ªÉm tra backend).")

# ---------------- TAB 2: VIDEO / WEBCAM ----------------
elif choice == "Video/Webcam":
    st.header("Video/Webcam (ch·∫°y model local)")
    st.markdown(
        "**L∆∞u √Ω:** tab n√†y d√πng model local (yolov8) ƒë·ªÉ demo video/webcam. "
        "ƒê·∫£m b·∫£o `yolov8/best.pt` c√≥ s·∫µn n·∫øu mu·ªën ch·∫°y local inference."
    )

    # Checkbox ch·ªçn ch·∫°y local inference
    use_local = st.checkbox("Ch·∫°y inference local (kh√¥ng qua API)", value=True, key="local_inference_toggle")

    if use_local:
        from ultralytics import YOLO

        @st.cache_resource
        def load_local_model():
            try:
                model_path = os.path.join(os.path.dirname(__file__), "..", "yolov8", "best.pt")
                return YOLO(model_path)
            except Exception as e:
                st.error(f"Kh√¥ng load ƒë∆∞·ª£c model local: {e}")
                return None

        local_model = load_local_model()
        if local_model is None:
            st.error("Kh√¥ng load ƒë∆∞·ª£c model local. Ki·ªÉm tra yolov8/best.pt")
            use_local = False
    else:
        st.info("S·ª≠ d·ª•ng FastAPI backend ƒë·ªÉ inference video (kh√¥ng kh·∫£ d·ª•ng cho webcam realtime).")

    source = st.radio("Ngu·ªìn:", ["Video file", "Webcam"], horizontal=True, key="video_source")
    conf_v = st.slider("Confidence", 0.1, 1.0, 0.5, 0.05, key="confidence_slider")

    frame_slot = st.empty()  # slot hi·ªÉn th·ªã frame

    # ---------------- Video File ----------------
    if source == "Video file":
        up = st.file_uploader("Upload video", type=["mp4", "mov", "avi"], key="video_upload")
        if up:
            t = tempfile.NamedTemporaryFile(delete=False)
            t.write(up.read())
            cap = cv2.VideoCapture(t.name)

            stop_video = st.checkbox("D·ª´ng video", value=False, key="stop_video_toggle")
            while cap.isOpened() and not stop_video:
                ret, frame = cap.read()
                if not ret:
                    break
                if use_local and local_model:
                    res = local_model.track(frame, conf=conf_v, persist=True, tracker="bytetrack.yaml")
                    frame = res[0].plot()
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame_slot.image(frame_rgb, use_container_width=True)

                stop_video = st.session_state.get("stop_video_toggle", False)

            cap.release()

    # ---------------- Webcam ----------------
    else:
        if use_local:
            # Checkbox b·∫≠t/t·∫Øt webcam (kh√¥ng t·∫°o nhi·ªÅu l·∫ßn)
            if "webcam_running" not in st.session_state:
                st.session_state.webcam_running = False
            st.session_state.webcam_running = st.checkbox(
                "B·∫≠t webcam", value=False, key="webcam_toggle"
            )

            if st.session_state.webcam_running:
                cap = cv2.VideoCapture(0)
                while st.session_state.webcam_running:
                    ret, frame = cap.read()
                    if not ret:
                        st.warning("Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c webcam.")
                        break
                    res = local_model.track(frame, conf=conf_v, persist=True, tracker="bytetrack.yaml")
                    frame = res[0].plot()
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frame_slot.image(frame_rgb, use_container_width=True)

                    # Ki·ªÉm tra tr·∫°ng th√°i checkbox
                    if not st.session_state.webcam_running:
                        break

                cap.release()
        else:
            st.error("Webcam realtime y√™u c·∫ßu inference local (Model local). B·∫≠t 'Ch·∫°y inference local' tr∆∞·ªõc.")

# ---------------- TAB 3: M√î H√åNH & TH·ªêNG K√ä ----------------
elif choice == "Th·ªëng k√™":
    st.header("M√¥ h√¨nh & Th·ªëng k√™")
    st.markdown("""
    **M√¥ h√¨nh hi·ªÉn th·ªã:** M√¥ h√¨nh YOLOv8 nh·∫≠n d·∫°ng m√≠t ch√≠n  
    **T·∫≠p d·ªØ li·ªáu:** ~1700 ·∫£nh, 3 nh√£n: mit_non, mit_chin, mit_saubenh  
    **Tri·ªÉn khai:** FastAPI (inference) + Streamlit (frontend)  
    """)
    st.markdown("B·∫°n c√≥ th·ªÉ m·ªü tab So s√°nh ƒë·ªÉ xem k·∫øt qu·∫£ training (results_n.csv / results_s.csv).")

# ---------------- TAB 4: So s√°nh ----------------
elif choice == "So s√°nh YOLOv8":
    st.header("So s√°nh YOLOv8n vs YOLOv8s")
    path_n = os.path.join(os.path.dirname(__file__), "..", "yolov8", "results_n.csv")
    path_s = os.path.join(os.path.dirname(__file__), "..", "yolov8", "results_s.csv")
    if os.path.exists(path_n) and os.path.exists(path_s):
        df_n = pd.read_csv(path_n)
        df_s = pd.read_csv(path_s)
        metrics = ["metrics/precision(B)", "metrics/recall(B)", "metrics/mAP50(B)"]
        for m in metrics:
            if m in df_n.columns and m in df_s.columns:
                fig, ax = plt.subplots()
                ax.plot(df_n.index, df_n[m], label="n")
                ax.plot(df_s.index, df_s[m], label="s")
                ax.set_title(m)
                ax.legend()
                st.pyplot(fig)
        st.dataframe({
            "YOLOv8n (last)": df_n.iloc[-1].to_dict(),
            "YOLOv8s (last)": df_s.iloc[-1].to_dict()
        })
    else:
        st.warning("Thi·∫øu results_n.csv / results_s.csv trong yolov8/ ‚Äî export t·ª´ qu√° tr√¨nh training.")

# ---------------- TAB 5: AI INSIGHT (Gemini ph√¢n t√≠ch k·∫øt qu·∫£) ----------------
elif choice == "AI Insight":
    st.header("AI Insight ‚Äî Ph√¢n t√≠ch k·∫øt qu·∫£")
    st.markdown("Tab n√†y ƒë·ªçc file `latest_results.json` (do backend l∆∞u) v√† d√πng Gemini ƒë·ªÉ sinh b√°o c√°o k·ªπ thu·∫≠t. N·∫øu Gemini kh√¥ng s·∫µn, app d√πng fallback ph√¢n t√≠ch t·ª± ƒë·ªông ng·∫Øn.")

    if os.path.exists(LATEST_RESULTS):
        with open(LATEST_RESULTS, "r", encoding="utf-8") as f:
            last = json.load(f)
        st.subheader("K·∫øt qu·∫£ m·ªõi nh·∫•t")
        st.json(last)
        if st.button("Y√™u c·∫ßu AI ph√¢n t√≠ch (Gemini)"):
            # build prompt
            counts = last.get("counts", {})
            total = sum(counts.values()) if counts else 0
            prompt = f"""B·∫°n l√† chuy√™n gia n√¥ng nghi·ªáp + k·ªπ s∆∞ AI. D·ªØ li·ªáu ƒë·∫ßu v√†o t·ª´ h·ªá th·ªëng nh·∫≠n d·∫°ng tr√°i m√≠t:
counts={counts}, total={total}.
H√£y vi·∫øt b√°o c√°o k·ªπ thu·∫≠t (ti·∫øng Vi·ªát, formal) g·ªìm:
1) T√≥m t·∫Øt t√¨nh tr·∫°ng (t·ªâ l·ªá ch√≠n/non/s√¢u b·ªánh)
2) Khuy·∫øn ngh·ªã thu ho·∫°ch (khi n√†o, v√¨ sao)
3) Bi·ªán ph√°p x·ª≠ l√Ω n·∫øu th·∫•y s√¢u b·ªánh
4) G·ª£i √Ω k·ªπ thu·∫≠t cho tri·ªÉn khai ti·∫øp (sampling, thu th·∫≠p th√™m ·∫£nh)
5) Ng·∫Øn g·ªçn k·∫øt lu·∫≠n.
Tr·∫£ l·ªùi chi ti·∫øt, ng√¥n ng·ªØ chuy√™n m√¥n k·ªπ thu·∫≠t n√¥ng nghi·ªáp v√† AI."""
            st.info("ƒêang g·ª≠i prompt ƒë·∫øn Gemini (n·∫øu key h·ª£p l·ªá)...")
            try:
                if GEMINI_KEY:
                    model = genai.GenerativeModel("models/gemini-2.5-flash")
                    resp = model.generate_content(prompt)
                    ai_text = getattr(resp, "text", None) or str(resp)
                else:
                    raise RuntimeError("No GEMINI key")
            except Exception as e:
                ai_text = None
                st.error(f"Kh√¥ng g·ªçi ƒë∆∞·ª£c Gemini (fallback). L·ªói: {e}")

            if not ai_text:
                # fallback local analysis
                lines = []
                lines.append("B√°o c√°o ph√¢n t√≠ch (fallback):")
                if total == 0:
                    lines.append("- Kh√¥ng ph√°t hi·ªán tr√°i m√≠t n√†o trong ·∫£nh.")
                else:
                    for k, v in counts.items():
                        pct = (v/total)*100 if total>0 else 0
                        lines.append(f"- {k}: {v} tr√°i ({pct:.1f}%)")
                    if counts.get("mit_chin",0)/total > 0.7:
                        lines.append("- Khuy·∫øn ngh·ªã: Thu ho·∫°ch s·ªõm trong 1-3 ng√†y.")
                    elif counts.get("mit_chin",0)/total > 0.4:
                        lines.append("- Khuy·∫øn ngh·ªã: Theo d√µi, c√≥ th·ªÉ thu ho·∫°ch l·ª©a nh·ªè.")
                    else:
                        lines.append("- Khuy·∫øn ngh·ªã: Ch∆∞a thu ho·∫°ch; ti·∫øp t·ª•c theo d√µi.")
                ai_text = "\n".join(lines)
            st.subheader("K·∫øt qu·∫£ ph√¢n t√≠ch AI")
            st.markdown(ai_text)
    else:
        st.info("Ch∆∞a c√≥ k·∫øt qu·∫£ m·ªõi (latest_results.json). H√£y ch·∫°y Tab ·∫¢nh ƒë·ªÉ t·∫°o.")

# ---------------- TAB 6: CHAT (Gemini) ----------------
elif choice == "Chat Gemini":
    st.header("Chat t·ª± do v·ªõi Gemini")
    st.header("Chat t·ª± do v·ªõi Gemini (h·ªèi v·ªÅ m√¥ h√¨nh, n√¥ng nghi·ªáp...)")
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    for msg in st.session_state.chat_history:
        role = msg["role"]
        content = msg["content"]
        if role=="user":
            st.write(f"**B·∫°n:** {content}")
        else:
            st.write(f"**AI:** {content}")

    user_q = st.text_input("Nh·∫≠p c√¢u h·ªèi", "")
    if st.button("G·ª≠i c√¢u h·ªèi"):
        if user_q.strip():
            st.session_state.chat_history.append({"role":"user","content":user_q})
            try:
                if GEMINI_KEY:
                    model = genai.GenerativeModel("models/gemini-2.5-flash")
                    resp = model.generate_content(user_q)
                    answer = getattr(resp, "text", None) or str(resp)
                else:
                    raise RuntimeError("No GEMINI key")
            except Exception as e:
                answer = f"[Fallback tr·∫£ l·ªùi t·ª± ƒë·ªông] Kh√¥ng th·ªÉ g·ªçi Gemini: {e}"
            st.session_state.chat_history.append({"role":"assistant","content":answer})
